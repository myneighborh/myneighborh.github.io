---
title: "A Structured Overview of the Neural Network Training Pipeline"
date: 2024-03-29
category: [deeplearning]
header:
  teaser: https://github.com/user-attachments/assets/d161e7d1-c2a0-40b1-8e17-852eef499b2b
---
<img src="https://github.com/user-attachments/assets/d161e7d1-c2a0-40b1-8e17-852eef499b2b" width="300"/>

## 1. 처음에는 **무작위 가중치(W, b)로 시작**
- 모델은 초기 상태에서 아무것도 모르므로, 가중치 `W`와 편향 `b`를 **랜덤하게 설정**.
- 따라서 처음의 선형 변환 결과는 **의미 없는 값이 나옴**.

---

## 2. 입력 데이터에 대해 **Affine Transformation(선형 변환)과 Activation 함수**를 적용
- 수식:  ŷ = σ(Wx + b)
- 입력 `x`에 대해 현재의 가중치(W)와 bias(b)로 Affine Transformation한 후 Activation Function층을 거쳐 **예측값 `ŷ`를 계산함**.
- 이 과정을 **순전파(Forward Propagation)**라고 부름.
  
---

## 3. **Loss 함수를 통해 예측값과 실제값을 비교함**
- 예: MSE, CrossEntropy 등  
Loss = difference(y, ŷ)

- Loss 값이 클수록 **가중치가 잘못되었음을 의미함**.

---

## 4. **역전파(Backpropagation)를 통해 가중치를 조정함**
- Loss 값을 줄이기 위해 `W`, `b`에 대한 미분 값을 계산하여 업데이트함.
- 이 과정을 **경사 하강법(Gradient Descent)**이라고 부름.
- 여기서 `η`는 **학습률(learning rate)**을 의미함.    
W ← W - η ⋅ ∂Loss/∂W

---

## 5. 조정된 가중치로 **다시 예측을 수행함**
- 업데이트된 `W`, `b`를 사용하여 다시 예측을 진행함.
- 이 과정을 **반복하면서 점점 더 정확한 모델로 발전함**.



